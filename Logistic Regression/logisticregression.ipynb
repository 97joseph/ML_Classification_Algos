{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It is a classification not a regression algorithm. \n",
    "It is used to estimate discrete values ( Binary values like 0/1, yes/no, true/false )\n",
    "based on given set of independent variable(s). \n",
    "In simple words, it predicts the probability of occurrence of an event by fitting data to a logit function(the sigmoid function).\n",
    "The Sigmoid function is of the form \n",
    "\n",
    "y = 1 - (1 / (1 + exp(intercept + (coef * x))))\n",
    "\n",
    "Hence, it is also known as logit regression.\n",
    "Since, it predicts the probability, its output values lies between 0 and 1 (as expected).\n",
    "\n",
    "\n",
    "Coming to the math, the log odds of the outcome is modeled as a linear combination of the predictor variables.\n",
    "\n",
    "odds= p/ (1-p) = probability of event occurrence / probability of not event occurrence\n",
    "ln(odds) = ln(p/(1-p))\n",
    "logit(p) = ln(p/(1-p)) = b0+b1X1+b2X2+b3X3....+bkXk\n",
    "\n",
    "NB/  *Logistic Regression is a Machine Learning algorithm which is used for the binary classification problems, \n",
    "      it is a predictive analysis algorithm and based on the concept of probability.\n",
    "     *Logistic Regression can be used for various classification problems such as spam detection.\n",
    "     *Logistic Regression is one of the most simple and commonly used Machine Learning algorithms for two-class classification.\n",
    "     *It is the go-to method for binary classification problems (problems with two class values).\n",
    "     *Logistic regression is named for the function used at the core of the method, the logistic function (sigmoid function).\n",
    "     *sigmoid function is an S-shaped curve that can take any real-valued number and map it into a value between 0 and 1,\n",
    "     but never exactly at those limits.\n",
    "\n",
    "     1 / (1 + e^-value)\n",
    "        \n",
    "Prepare Data for Logistic Regression\n",
    "\n",
    "The assumptions made by logistic regression about the distribution and relationships in your data are much the same as the assumptions made in linear regression.\n",
    "\n",
    "Much study has gone into defining these assumptions and precise probabilistic and statistical language is used. My advice is to use these as guidelines or rules of thumb and experiment with different data preparation schemes.\n",
    "\n",
    "Ultimately in predictive modeling machine learning projects you are laser focused on making accurate predictions rather than interpreting the results. As such, you can break some assumptions as long as the model is robust and performs well.\n",
    "\n",
    "\n",
    "Binary Output Variable: This might be obvious as we have already mentioned it, but logistic regression is intended for binary (two-class) classification problems. It will predict the probability of an instance belonging to the default class, which can be snapped into a 0 or 1 classification.\n",
    "\n",
    "Remove Noise: Logistic regression assumes no error in the output variable (y), consider removing outliers and possibly misclassified instances from your training data.\n",
    "\n",
    "Gaussian Distribution: Logistic regression is a linear algorithm (with a non-linear transform on output). It does assume a linear relationship between the input variables with the output. Data transforms of your input variables that better expose this linear relationship can result in a more accurate model. For example, you can use log, root, Box-Cox and other univariate transforms to better expose this relationship.\n",
    "\n",
    "Remove Correlated Inputs: Like linear regression, the model can overfit if you have multiple highly-correlated inputs. Consider calculating the pairwise correlations between all inputs and removing highly correlated inputs.\n",
    "\n",
    "Fail to Converge: It is possible for the expected likelihood estimation process that learns the coefficients to fail to converge. This can happen if there are many highly correlated inputs in your data or the data is very sparse (e.g. lots of zeros in your input data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "code example\n",
    "\"\"\"\n",
    "\n",
    "# importing required libraries\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# read the train and test dataset\n",
    "train_data = pd.read_csv('train-data.csv')\n",
    "test_data = pd.read_csv('test-data.csv')\n",
    "\n",
    "\n",
    "print(train_data.head())\n",
    "\n",
    "# shape of the dataset\n",
    "print('Shape of training data :',train_data.shape)\n",
    "print('Shape of testing data :',test_data.shape)\n",
    "\n",
    "# Now, we need to predict the missing target variable in the test data\n",
    "# target variable - Survived\n",
    "\n",
    "# seperate the independent and target variable on training data\n",
    "train_x = train_data.drop(columns=['Survived'],axis=1)\n",
    "train_y = train_data['Survived']\n",
    "\n",
    "# seperate the independent and target variable on testing data\n",
    "test_x = test_data.drop(columns=['Survived'],axis=1)\n",
    "test_y = test_data['Survived']\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(train_x,train_y)\n",
    "\n",
    "# coefficeints of the trained model\n",
    "print('Coefficient of model :', model.coef_)\n",
    "\n",
    "# intercept of the model\n",
    "print('Intercept of model',model.intercept_)\n",
    "\n",
    "# predict the target on the train dataset\n",
    "predict_train = model.predict(train_x)\n",
    "print('Target on train data',predict_train) \n",
    "\n",
    "# Accuray Score on train dataset\n",
    "accuracy_train = accuracy_score(train_y,predict_train)\n",
    "print('accuracy_score on train dataset : ', accuracy_train)\n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_test = model.predict(test_x)\n",
    "print('Target on test data',predict_test) \n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "accuracy_test = accuracy_score(test_y,predict_test)\n",
    "print('accuracy_score on test dataset : ', accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Working out an example using pima indian diabetes dataset\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sn\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "column_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
    "\n",
    "# load dataset\n",
    "pima = pd.read_csv(\"pima-indians-diabetes.csv\", header=None, names=column_names)\n",
    "\n",
    "\n",
    "#split dataset in features and target variable\n",
    "feature_columns = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree']\n",
    "X = pima[feature_columns] # Features\n",
    "y = pima.label # Target variable\n",
    "\n",
    "# split X and y into training and testing sets with our test data taking 25% & train data 75%\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "\n",
    "pima.head()\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "#print shape of our dataset\n",
    "print('Shape of training data:',X_train.shape)\n",
    "print('Shape of testing data :',X_test.shape)\n",
    "\n",
    "#confusion matrix\n",
    "confusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "sn.heatmap(confusion_matrix, annot=True)\n",
    "\n",
    "print('Accuracy: ',metrics.accuracy_score(y_test, y_pred))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
