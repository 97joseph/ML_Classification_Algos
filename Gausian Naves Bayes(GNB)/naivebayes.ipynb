{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Naive Bayes\n",
    "It is a classification technique based on Bayes’ theorem with an assumption of independence between predictors. \n",
    "In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is \n",
    "unrelated to the presence of any other feature. \n",
    "For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. \n",
    "Even if these features depend on each other or upon the existence of the other features, \n",
    "a naive Bayes classifier would consider all of these properties to independently contribute to the probability that this fruit is an apple.\n",
    "\n",
    "Naive Bayesian model is easy to build and particularly useful for very large data sets. \n",
    "Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.\n",
    " \n",
    "Bayes theorem provides a way of calculating posterior probability P(c|x) from P(c), P(x) and P(x|c). \n",
    "Look at the equation below:\n",
    "Bayes_rule\n",
    "\n",
    "\n",
    "Here,\n",
    "\n",
    "P(c|x) is the posterior probability of class (target) given predictor (attribute).\n",
    "P(c) is the prior probability of class.\n",
    "P(x|c) is the likelihood which is the probability of predictor given class.\n",
    "P(x) is the prior probability of predictor.\n",
    "Example: Let’s understand it using an example. Below I have a training data set of weather and corresponding target variable ‘Play’. \n",
    "    Now, we need to classify whether players will play or not based on weather condition. Let’s follow the below steps to perform it.\n",
    "\n",
    "Step 1: Convert the data set to frequency table\n",
    "\n",
    "Step 2: Create Likelihood table by finding the probabilities like Overcast probability = 0.29 and probability of playing is 0.64.\n",
    "\n",
    "Bayes_4\n",
    "\n",
    "Step 3: Now, use Naive Bayesian equation to calculate the posterior probability for each class. \n",
    "    The class with the highest posterior probability is the outcome of prediction.\n",
    "\n",
    "Problem: Players will play if weather is sunny, is this statement is correct?\n",
    "\n",
    "We can solve it using above discussed method, so P(Yes | Sunny) = (P( Sunny | Yes) * P(Yes)) / P (Sunny)\n",
    "\n",
    "Here we have P (Sunny | Yes) = 3/9 = 0.33, P(Sunny) = 5/14 = 0.36, P( Yes)= 9/14 = 0.64\n",
    "\n",
    "Now, P (Yes | Sunny) = 0.33 * 0.64 / 0.36 = 0.60, which has higher probability.\n",
    "\n",
    "Naive Bayes uses a similar method to predict the probability of different class based on various attributes. \n",
    "This algorithm is mostly used in text classification and with problems having multiple classes.\n",
    "\n",
    "NB/ In Gaussian naive Bayes classifier, the assumption is that data from each label is drawn from a simple Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "\n",
    "column_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
    "\n",
    "dataset = pd.read_csv('pima-indians-diabetes.csv', header=None, names=column_names)\n",
    "\n",
    "print(dataset.describe(), sep=\"\\n\")\n",
    "X = dataset.iloc[:, 0:8]\n",
    "y = dataset.iloc[:, 8]\n",
    "\n",
    "## split X and y into training and testing sets with our test data taking 25% & train data 75%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.25)\n",
    "\n",
    "#view correlation\n",
    "sn.heatmap(X.corr(), annot = True)\n",
    "\n",
    "#create a model & fit it\n",
    "model = GaussianNB().fit(X_train, y_train)\n",
    "\n",
    "#prediction\n",
    "y_pred = model.predict(X_test)\n",
    "# print(y_pred)\n",
    "\n",
    "#Feature scaling\n",
    "scale_X = StandardScaler()\n",
    "X_train = scale_X.fit_transform(X_train)\n",
    "X_test = scale_X.transform(X_test)\n",
    "\n",
    "#model evaluation\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print (confusion_matrix)\n",
    "print(f1_score(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "dataset.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
